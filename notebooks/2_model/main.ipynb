{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "71ccff01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training_folder': '../../scenarios/clermont_v2', 'params': {'general': {'step_size': '0.001', 'use_road_network': True, 'coef_day_to_year': '300', 'clustering_radius': '500'}}}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "\n",
    "general = {'step_size': '0.001', 'use_road_network': True, 'coef_day_to_year': '300', 'clustering_radius': '500'}\n",
    "\n",
    "params = {\n",
    "    'general': general,\n",
    "    }\n",
    "\n",
    "default = {'training_folder': '../../scenarios/clermont_v2', 'params':params} # Default execution parameters\n",
    "manual, argv = (True, default) if 'ipykernel' in sys.argv[0] else (False, dict(default, **json.loads(sys.argv[1])))\n",
    "print(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "99b5db61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numba threads 16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50)\n",
    "sys.path.insert(0, r'../../../quetzal') # Add path to quetzal\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point, LineString\n",
    "from typing import Literal\n",
    "import numba as nb\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import DBSCAN\n",
    "import shapely\n",
    "#num_cores = 1\n",
    "print('numba threads',nb.config.NUMBA_NUM_THREADS)\n",
    "\n",
    "on_lambda = bool(os.environ.get('AWS_EXECUTION_ENV'))\n",
    "io_engine = 'pyogrio' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "772a187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, r'../../') # Add path\n",
    "from utils import get_epsg, population_to_mesh, get_acf_distances, get_routing_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c740a4",
   "metadata": {},
   "source": [
    "# Folders stucture and params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbbcb7a",
   "metadata": {},
   "source": [
    "Everything is on S3 (nothing on ECR) so no direct input folder. just scenarios/{scen}/inputs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "8ade8441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../scenarios/clermont_v2'"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argv['training_folder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "d16ff565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'general': {'step_size': '0.001',\n",
       "  'use_road_network': True,\n",
       "  'coef_day_to_year': '300',\n",
       "  'clustering_radius': '500'}}"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argv['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "4e405b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = argv['training_folder']\n",
    "input_folder = os.path.join(base_folder,'inputs/')\n",
    "pt_folder  = os.path.join(input_folder,'pt/')\n",
    "road_folder = os.path.join(input_folder,'road/') ## réseau ferré\n",
    "od_folder =  os.path.join(input_folder,'od/')\n",
    "\n",
    "output_folder = os.path.join(base_folder,'outputs/')\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "model_folder = os.path.join(input_folder, 'model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "81b11c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read general params\n",
    "step_size_min = 0.0005\n",
    "step_size = max(float(argv['params']['general'].get('step_size')), step_size_min)\n",
    "use_road_network = argv['params']['general'].get('use_road_network') ## param use r_ntw\n",
    "coef_day_to_year = float(argv['params']['general'].get('coef_day_to_year'))\n",
    "clustering_radius = float(argv['params']['general'].get('clustering_radius'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "9d656677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default pt_links params in case not filled by the user\n",
    "default_catchment_radius = 500      # meters\n",
    "default_capex = 0.3                 # €/veh.km\n",
    "default_capacity = 60               # vehicle capacity in PAX\n",
    "default_service_hours = 12          # nb d'heures par jour de fonctionnement\n",
    "default_road_pt_factor = 1.25       # facteur de dilatation des vitesses routières pour les TC utilisant la route (bus, bus express...)\n",
    "default_stop_interval = 300         # temps d'arrêt à chaque arrêt\n",
    "default_modal_share = 0.2           # modal share for the mode (used only if ODs are provided)\n",
    "#TODO: voir si on peut ajouter les champs défaut dans les liens ==> suppr. ==> adapter script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd032bd",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bacf9ca",
   "metadata": {},
   "source": [
    "PT links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "5b5ada42",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pt_folder + 'links.geojson') as f:\n",
    "    links_ = json.load(f)\n",
    "\n",
    "columns = set()\n",
    "for feature in links_['features']:\n",
    "    for key in feature['properties'].keys():\n",
    "        columns.add(key)\n",
    "\n",
    "links = pd.DataFrame(links_['features'])\n",
    "for col in columns:\n",
    "    links[col] = links.apply(lambda x: x['properties'].get(col, None), 1)\n",
    "links['geometry'] = links['geometry'].apply(lambda x: LineString(x['coordinates']))\n",
    "links.drop(columns=['type', 'properties'], inplace=True)\n",
    "\n",
    "links = links.set_index('index')\n",
    "links = gpd.GeoDataFrame(links, geometry='geometry', crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "1a674100",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = gpd.read_file(pt_folder + 'nodes.geojson', engine=io_engine)\n",
    "nodes = nodes.set_index('index')\n",
    "nodes = nodes[~pd.isna(nodes.geometry)]\n",
    "\n",
    "nodes['nindex'] = nodes.reset_index().index\n",
    "nodes['stop_name'] = nodes.apply(lambda x: x['nindex'] if (pd.isna(x['stop_name']) or x['stop_name'] is None) else x['stop_name'], 1).astype(str)\n",
    "nodes.drop(columns='nindex', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "2b3d3c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_nodes_to_route_id_a = links.copy()[['a', 'trip_id', 'route_id']]\n",
    "prep_nodes_to_route_id_b = links.copy()[['b', 'trip_id', 'route_id']]\n",
    "prep_nodes_to_route_id_a.rename(columns={'a': 'node'}, inplace=True)\n",
    "prep_nodes_to_route_id_b.rename(columns={'b': 'node'}, inplace=True)\n",
    "\n",
    "prep_nodes_to_route_id = pd.concat([prep_nodes_to_route_id_a, prep_nodes_to_route_id_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "6d638802",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_to_trip_ids = prep_nodes_to_route_id.groupby('node')['trip_id'].agg(set).to_dict()\n",
    "nodes_to_route_ids = prep_nodes_to_route_id.groupby('node')['route_id'].agg(set).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "89a5ac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes['trip_ids'] = nodes.index.map(nodes_to_trip_ids)\n",
    "nodes['route_ids'] = nodes.index.map(nodes_to_route_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "21d54c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'capacity' not in links.columns:\n",
    "    links['capacity'] = default_capacity\n",
    "else:\n",
    "    links['capacity'] = links['capacity'].fillna(default_capacity).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "921a0d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'capex' not in links.columns:\n",
    "    links['capex'] = default_capex\n",
    "else:\n",
    "    links['capex'] = links['capex'].fillna(default_capex).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "daa5b7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_radii = [x for x in links.columns if 'catchment_radius' in x]\n",
    "catchment_radii_provided = (len(catchment_radii) > 0)\n",
    "if catchment_radii_provided:\n",
    "    for x in catchment_radii:\n",
    "        links[x] = links[x].astype(float)\n",
    "        M = links[x].max()\n",
    "        if not links[x].equals(links[x].fillna(M)):\n",
    "            print('!! Catchemnt radius values missing in column {} !!'.format(x))\n",
    "            links[x] = links[x].fillna(M)\n",
    "else:\n",
    "    links['catchment_radius'] = default_catchment_radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "642dc9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'nb_service_hours' not in links.columns:\n",
    "    links['nb_service_hours'] = default_service_hours\n",
    "else:\n",
    "    links['nb_service_hours'] = links['nb_service_hours'].fillna(default_service_hours).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "47cc208d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'departures' not in links.columns:\n",
    "    links['departures'] = None\n",
    "if 'arrivals' not in links.columns:\n",
    "    links['arrivals'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "adc303a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'road_pt_factor' not in links.columns:\n",
    "    links['road_pt_factor'] = default_road_pt_factor\n",
    "links['road_pt_factor'] = links['road_pt_factor'].fillna(default_road_pt_factor)\n",
    "\n",
    "if 'stop_interval' not in links.columns:\n",
    "    links['stop_interval'] = default_stop_interval\n",
    "links['stop_interval'] = links['stop_interval'].fillna(default_stop_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331a0486",
   "metadata": {},
   "source": [
    "Input data zoning file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "0d6324e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32631"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find meters CRS\n",
    "centroid = [*LineString(nodes.centroid.values).centroid.coords][0]\n",
    "crs = get_epsg(centroid[1],centroid[0])\n",
    "crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "99e4e06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "zonage_file = os.path.join(input_folder, 'zonage.geojson')\n",
    "zonage_file_provided = os.path.isfile(zonage_file)\n",
    "if zonage_file_provided :\n",
    "    zonage_ = gpd.read_file(input_folder + 'zonage.geojson', engine=io_engine).to_crs(epsg='4326')\n",
    "    zonage_['area (km2)'] = zonage_.to_crs(crs).area / 10**6\n",
    "    if 'zone_id' not in zonage_.columns:\n",
    "        zonage_.reset_index(names='zone_id', inplace=True)\n",
    "else:\n",
    "    print('No zonage file in the input folder...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "1f0eef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_with_zones = nodes.copy().sjoin(zonage_, predicate='intersects', how='left')[['stop_name', 'route_ids', 'trip_ids', 'zone_id']]\n",
    "nodes_with_zones['route_ids'] = nodes_with_zones['route_ids'].apply(lambda x: list(x))\n",
    "nodes_with_zones['trip_ids'] = nodes_with_zones['trip_ids'].apply(lambda x: list(x))\n",
    "nodes_with_zones.reset_index(inplace=True)\n",
    "nodes_per_zone = nodes_with_zones.groupby('zone_id')[['stop_name', 'route_ids', 'trip_ids', 'index']].agg({'stop_name': set, 'route_ids': sum, 'trip_ids': sum, 'index': set}).to_dict()\n",
    "\n",
    "zonage = zonage_.copy()\n",
    "zonage['route_ids'] = zonage['zone_id'].map(nodes_per_zone['route_ids']).apply(lambda x: (x if isinstance(x, set) else set(x)) if not isinstance(x, float) else set())\n",
    "zonage['trip_ids'] = zonage['zone_id'].map(nodes_per_zone['trip_ids']).apply(lambda x: (x if isinstance(x, set) else set(x)) if not isinstance(x, float) else set())\n",
    "zonage['stop_names'] = zonage['zone_id'].map(nodes_per_zone['stop_name']).apply(lambda x: (x if isinstance(x, set) else set(x)) if not isinstance(x, float) else set())\n",
    "zonage['stop_ids'] = zonage['zone_id'].map(nodes_per_zone['index']).apply(lambda x: (x if isinstance(x, set) else set(x)) if not isinstance(x, float) else set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "4ad58e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "densities = [x for x in zonage.columns if 'density' in x]\n",
    "assert len(densities) > 0, 'Please provide densities as input data in the zoning file'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "ece865d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "zonage.to_crs(epsg=4326).to_file(output_folder + 'zoning.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "36a141e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_ph_columns = ('headway_ph' in links.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69419eeb",
   "metadata": {},
   "source": [
    "Road network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "ac965c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "road network ? True\n"
     ]
    }
   ],
   "source": [
    "## road network here\n",
    "rnodes_file = os.path.join(road_folder, 'road_nodes.geojson')\n",
    "rnodes_file_provided = os.path.isfile(rnodes_file)\n",
    "if rnodes_file_provided:\n",
    "    rnodes = gpd.read_file(os.path.join(road_folder, 'road_nodes.geojson'), engine=io_engine)\n",
    "    rnodes = rnodes.set_index('index').to_crs(epsg='4326')\n",
    "    rlinks = gpd.read_file(os.path.join(road_folder, 'road_links.geojson'), engine=io_engine)\n",
    "    rlinks = rlinks.set_index('index').to_crs(epsg='4326')\n",
    "print('road network ?',rnodes_file_provided)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d214b5b",
   "metadata": {},
   "source": [
    "OD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "6071cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "od_file = os.path.join(od_folder, 'od.geojson')\n",
    "od_file_provided = os.path.isfile(od_file)\n",
    "if od_file_provided:\n",
    "    od = gpd.read_file(od_file, engine=io_engine)\n",
    "    if 'index' not in od.columns:\n",
    "        od.reset_index(names='index', inplace=True)\n",
    "    if 'name' not in od.columns:\n",
    "        od['name'] = od['index'].astype(str)\n",
    "    od['name'] = od['name'].fillna(od['index'].astype(str))\n",
    "\n",
    "    od.drop_duplicates(inplace=True)\n",
    "else:\n",
    "    print('OD?', od_file_provided)\n",
    "\n",
    "if od_file_provided:\n",
    "    if 'modal_share_input' not in links.columns:\n",
    "        links['modal_share_input'] = default_modal_share\n",
    "    links['modal_share_input'] = links['modal_share_input'].fillna(default_modal_share).astype(float)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0687074a",
   "metadata": {},
   "source": [
    "# Init result dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "4cf77ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_route_id = pd.DataFrame(index=links['route_id'].unique())\n",
    "df_route_id.index.name = 'route_id'\n",
    "df_route_id = df_route_id.reset_index()\n",
    "if display_ph_columns:   \n",
    "    df_route_id = df_route_id.merge(links[['route_id', 'route_type', 'capacity', 'headway', 'headway_ph', 'headway_oph', 'nb_peak_hours']], on='route_id', how='left')\n",
    "else:\n",
    "    df_route_id = df_route_id.merge(links[['route_id', 'route_type', 'capacity', 'headway']], on='route_id', how='left')\n",
    "df_route_id = df_route_id.rename(columns={'capacity': 'veh_capacity (PAX)'})\n",
    "df_route_id = df_route_id.drop_duplicates()\n",
    "df_route_id = df_route_id.set_index('route_id')\n",
    "\n",
    "df_trip_id = pd.DataFrame(index=links['trip_id'].unique())\n",
    "df_trip_id.index.name = 'trip_id'\n",
    "df_trip_id = df_trip_id.reset_index()\n",
    "if display_ph_columns:   \n",
    "    df_trip_id = df_trip_id.merge(links[['trip_id', 'route_id', 'route_type', 'capacity', 'headway', 'headway_ph', 'headway_oph', 'nb_peak_hours']], on='trip_id', how='left')\n",
    "else:\n",
    "    df_trip_id = df_trip_id.merge(links[['trip_id', 'route_id', 'route_type', 'capacity', 'headway']], on='trip_id', how='left')\n",
    "df_trip_id = df_trip_id.rename(columns={'capacity': 'veh_capacity (PAX)'})\n",
    "df_trip_id = df_trip_id.drop_duplicates()\n",
    "df_trip_id = df_trip_id.set_index('trip_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "dbac81bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_route_type = pd.DataFrame(index=links['route_type'].unique())\n",
    "df_route_type.index.name='route_type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "f5cc4bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure headways are consistent : one single headway for both way and return\n",
    "# Otherwise can't calculate KPIs later\n",
    "\n",
    "df_route_id = df_route_id[~df_route_id.index.duplicated(keep='first')]\n",
    "route_headway = dict(zip(df_route_id.index, df_route_id['headway']))\n",
    "links.headway = links.route_id.map(route_headway)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46210dac",
   "metadata": {},
   "source": [
    "# Geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "e17d1c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "geoms_trip_id = links.groupby('trip_id')['geometry'].agg(shapely.unary_union).to_dict()\n",
    "df_trip_id['geometry'] = geoms_trip_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "ff5541d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "geoms_route_id = links.copy()\n",
    "geoms_route_id['trip_number'] = geoms_route_id['trip_id'].apply(lambda x: x[-1])\n",
    "geoms_route_id = geoms_route_id.loc[geoms_route_id['trip_number'] =='0']\n",
    "geoms_route_id = geoms_route_id.groupby('route_id')['geometry'].agg(shapely.unary_union).to_dict()\n",
    "df_route_id['geometry'] = geoms_route_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8fb983",
   "metadata": {},
   "source": [
    "# Catchment calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "2d54b3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_catchment_by_mode(col='route_id', pop_col='population', node_dist=None):\n",
    "    #get all nodes with col filter\n",
    "    link = links.groupby(col)[['a','b','route_type', 'catchment_radius']].agg({'a':set,'b':set,'route_type':'first', 'catchment_radius': 'first'})\n",
    "    link['node'] = link.apply(lambda row: row['a'].union(row['b']), axis=1)\n",
    "    link = link.drop(columns=['a','b'])\n",
    "\n",
    "    col_exist = col == 'route_type' # cannot explode if index == route_type (a column)\n",
    "    link = link.explode('node').reset_index(drop=col_exist)\n",
    "    link = node_dist.merge(link, left_on='node_index', right_on='node')\n",
    "    #filter by distance\n",
    "    link = link[link['distances'] <= link['catchment_radius']]\n",
    "    #drop duplicated mesh nodes (we count only one time)\n",
    "    link = link.drop_duplicates(subset=['mesh_index',col],keep='first')\n",
    "\n",
    "    return link.groupby(col)[pop_col].sum().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "8b150d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_catchment_by_access(col='route_id', pop_col='population', catchment_col='catchment_radius', node_dist=None):\n",
    "    #get all nodes with col filter\n",
    "    link = links.groupby(col)[['a','b','route_type', catchment_col]].agg({'a':set,'b':set,'route_type':'first', catchment_col:'first'})\n",
    "    link['node'] = link.apply(lambda row: row['a'].union(row['b']), axis=1)\n",
    "    link = link.drop(columns=['a','b'])\n",
    "\n",
    "    col_exist = col == 'route_type' # cannot explode if index == route_type (a column)\n",
    "    link = link.explode('node').reset_index(drop=col_exist)\n",
    "    link = node_dist.merge(link, left_on='node_index', right_on='node')\n",
    "    #filter by distance\n",
    "    # link[catchment_col] = link[catchment_col].astype(float)\n",
    "    link = link[link['distances'] <= link[catchment_col]]\n",
    "    #drop duplicated mesh nodes (we count only one time)\n",
    "    link = link.drop_duplicates(subset=['mesh_index',col],keep='first')\n",
    "\n",
    "    return link.groupby(col)[pop_col].sum().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "c438144c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "population\n",
      "using road_nodes\n",
      "178 nodes in multiple zones. will be match to a single zone.\n",
      "13 unfounded zones\n",
      "using road_nodes\n",
      "emplois\n",
      "using road_nodes\n",
      "178 nodes in multiple zones. will be match to a single zone.\n",
      "13 unfounded zones\n",
      "using road_nodes\n"
     ]
    }
   ],
   "source": [
    "meshes = {}\n",
    "node_dists = {}\n",
    "\n",
    "## road network here\n",
    "for density in densities:\n",
    "    if density == 'density' and 'population_density' not in densities:\n",
    "        tag = 'population'\n",
    "    elif density == 'density':\n",
    "        tag = 'x'\n",
    "    else:\n",
    "        tag = density.split('_density')[0]\n",
    "\n",
    "    print(tag)\n",
    "\n",
    "    zonage[tag] = zonage[density] * zonage['area (km2)']\n",
    "\n",
    "    if rnodes_file_provided and use_road_network:\n",
    "        # use rnodes as mesh_pop.\n",
    "        print('using road_nodes') ## road network here\n",
    "        mesh = population_to_mesh(zonage, mesh=rnodes, step=step_size, col=tag, fill_missing='closest')\n",
    "    else:\n",
    "        # create a mesh\n",
    "        mesh = population_to_mesh(zonage, mesh=None, step=step_size, col=tag, fill_missing='centroid')\n",
    "\n",
    "    #mesh.to_file(output_folder + 'population_mesh.geojson',driver='GeoJSON',engine=io_engine)\n",
    "    if catchment_radii_provided:\n",
    "        max_dist = (max([links[catchment_radius].max() for catchment_radius in catchment_radii]))\n",
    "    else:\n",
    "        max_dist = default_catchment_radius\n",
    "\n",
    "    meshes[tag] = mesh.copy()\n",
    "\n",
    "    # road network here, what to use if not road network?\n",
    "    if rnodes_file_provided: \n",
    "        print('using road_nodes')\n",
    "        node_dist = get_routing_distances(nodes, rnodes, rlinks, mesh, tag, 'length', max_dist)\n",
    "    else:\n",
    "        node_dist = get_acf_distances(nodes, mesh, tag, crs, max_dist)\n",
    "\n",
    "    node_dists[tag] = node_dist.copy()\n",
    "    \n",
    "    if catchment_radii_provided:\n",
    "        for catchment_radius in catchment_radii:\n",
    "            suf = catchment_radius.split('catchment_radius_')[1]\n",
    "\n",
    "            res_trip = get_catchment_by_access('trip_id', tag, catchment_radius, node_dist)\n",
    "            res_route = get_catchment_by_access('route_id', tag, catchment_radius, node_dist)\n",
    "            res_mode = get_catchment_by_access('route_type', tag, catchment_radius, node_dist)\n",
    "\n",
    "            if suf == '':\n",
    "                df_trip_id['catchment {}'.format(tag)] = res_trip\n",
    "                df_trip_id['catchment {}'.format(tag)] = df_trip_id['catchment {}'.format(tag)].fillna(0)\n",
    "                df_trip_id['catchment {}'.format(tag)] = df_trip_id['catchment {}'.format(tag)].apply(lambda x: round(x, 0)) \n",
    "\n",
    "                df_route_id['catchment {}'.format(tag)] = res_route\n",
    "                df_route_id['catchment {}'.format(tag)] = df_route_id['catchment {}'.format(tag)].fillna(0)\n",
    "                df_route_id['catchment {}'.format(tag)] = df_route_id['catchment {}'.format(tag)].apply(lambda x: round(x, 0)) \n",
    "\n",
    "                df_route_type['catchment {}'.format(tag)] = res_mode\n",
    "                df_route_type['catchment {}'.format(tag)] = df_route_type['catchment {}'.format(tag)].fillna(0)\n",
    "                df_route_type['catchment {}'.format(tag)] = df_route_type['catchment {}'.format(tag)].apply(lambda x: round(x, 0)) \n",
    "\n",
    "            else:\n",
    "                df_trip_id['catchment {} {}'.format(tag, suf)] = res_trip\n",
    "                df_trip_id['catchment {} {}'.format(tag, suf)] = df_trip_id['catchment {} {}'.format(tag, suf)].fillna(0)\n",
    "                df_trip_id['catchment {} {}'.format(tag, suf)] = df_trip_id['catchment {} {}'.format(tag, suf)].apply(lambda x: round(x, 0))\n",
    "\n",
    "                df_route_id['catchment {} {}'.format(tag, suf)] = res_route\n",
    "                df_route_id['catchment {} {}'.format(tag, suf)] = df_route_id['catchment {} {}'.format(tag, suf)].fillna(0)\n",
    "                df_route_id['catchment {} {}'.format(tag, suf)] = df_route_id['catchment {} {}'.format(tag, suf)].apply(lambda x: round(x, 0))  \n",
    "\n",
    "                df_route_type['catchment {} {}'.format(tag, suf)] = res_mode\n",
    "                df_route_type['catchment {} {}'.format(tag, suf)] = df_route_type['catchment {} {}'.format(tag, suf)].fillna(0)\n",
    "                df_route_type['catchment {} {}'.format(tag, suf)] = df_route_type['catchment {} {}'.format(tag, suf)].apply(lambda x: round(x, 0)) \n",
    "\n",
    "    else:\n",
    "        res_trip = get_catchment_by_mode('trip_id', tag, node_dist)\n",
    "        res_route = get_catchment_by_mode('route_id', tag, node_dist)\n",
    "        res_mode = get_catchment_by_mode('route_type', tag, node_dist)\n",
    "\n",
    "        df_trip_id['catchment {}'.format(tag)] = res_trip\n",
    "        df_trip_id['catchment {}'.format(tag)] = df_trip_id['catchment {}'.format(tag)].fillna(0)\n",
    "        df_trip_id['catchment {} {}'.format(tag, suf)] = df_trip_id['catchment {} {}'.format(tag, suf)].apply(lambda x: round(x, 0)) \n",
    "\n",
    "        df_route_id['catchment {}'.format(tag)] = res_route\n",
    "        df_route_id['catchment {}'.format(tag)] = df_route_id['catchment {}'.format(tag)].fillna(0)\n",
    "        df_route_id['catchment {} {}'.format(tag, suf)] = df_route_id['catchment {} {}'.format(tag, suf)].apply(lambda x: round(x, 0)) \n",
    "\n",
    "        df_route_type['catchment {}'.format(tag)] = res_mode, 0\n",
    "        df_route_type['catchment {}'.format(tag)] = df_route_type['catchment {}'.format(tag)].fillna(0)\n",
    "        df_route_type['catchment {} {}'.format(tag, suf)] = df_route_type['catchment {} {}'.format(tag, suf)].apply(lambda x: round(x, 0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d12fd",
   "metadata": {},
   "source": [
    "# Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "c661a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'departures' in links.columns:\n",
    "    links['link_has_timetable'] = links['departures'].apply(lambda x: 0 if x is None else 1)\n",
    "    lines_with_timetable = links.groupby('route_id')['link_has_timetable'].min()\n",
    "    links['line_has_timetable'] = links['route_id'].map(lines_with_timetable)\n",
    "    links.drop(columns='link_has_timetable', inplace=True)\n",
    "else:\n",
    "    links['line_has_timetable'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "4f15f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "default = {'headway_ph': links['headway'].max(),\n",
    "           'headway_oph': links['headway'].max(),\n",
    "           'nb_service_hours': 14,\n",
    "           'nb_peak_hours': 14}\n",
    "\n",
    "dic_headway = links.groupby('route_id')['headway'].min()\n",
    "\n",
    "for col in ['headway_ph', 'headway_oph']:\n",
    "    if col not in links.columns:\n",
    "        links[col] = default[col]\n",
    "    else: \n",
    "        dic_col = links.groupby('route_id')[col].min()\n",
    "        links[col] = links['route_id'].map(dic_col)\n",
    "        links.loc[links[col].isna(), col] = links.loc[links[col].isna(), 'route_id'].map(dic_headway)\n",
    "        links[col] = links[col].astype(int)\n",
    "\n",
    "for col in ['nb_service_hours', 'nb_peak_hours']:\n",
    "    if col not in links.columns:\n",
    "        links[col] = default[col]\n",
    "    else:\n",
    "        links[col] = links[col]\n",
    "        dic_col = links.groupby('route_id')[col].max()\n",
    "        links[col] = links['route_id'].map(dic_col).fillna(default[col])\n",
    "        links[col] = links[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "f8db3069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def time_seconds(time_str):\n",
    "    time_obj = datetime.strptime(time_str, '%H:%M:%S')\n",
    "    return time_obj.hour * 3600 + time_obj.minute * 60 + time_obj.second\n",
    "\n",
    "def retrieve_avg_headway(departures):\n",
    "    deps_seconds = [time_seconds(t) for t in departures]\n",
    "    if len(deps_seconds) >= 1:\n",
    "        gaps = [deps_seconds[i] - deps_seconds[i-1] for i in range(1, len(deps_seconds))]\n",
    "        return int(np.average(gaps))\n",
    "    return None\n",
    "\n",
    "def retrieve_oph_headway(departures):\n",
    "    deps_seconds = [time_seconds(t) for t in departures]\n",
    "    if len(deps_seconds) >= 1:\n",
    "        gaps = [deps_seconds[i] - deps_seconds[i-1] for i in range(1, len(deps_seconds))]\n",
    "        return max(gaps)\n",
    "    return None\n",
    "\n",
    "def retrieve_ph_headway(departures):\n",
    "    deps_seconds = [time_seconds(t) for t in departures]\n",
    "    if len(deps_seconds) >= 1:\n",
    "        gaps = [deps_seconds[i] - deps_seconds[i-1] for i in range(1, len(deps_seconds))]\n",
    "        return min(gaps)\n",
    "    return None\n",
    "\n",
    "def retrieve_service_hours(departures):\n",
    "    deps_seconds = [time_seconds(t) for t in departures]\n",
    "    if len(deps_seconds) >= 1:\n",
    "        return (deps_seconds[-1] - deps_seconds[0])/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "81e40ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "links.loc[links['line_has_timetable'] == 1, 'frequency_per_day'] = links.loc[links['line_has_timetable'] == 1].apply(lambda x: len(x['departures']), 1)\n",
    "links.loc[links['line_has_timetable'] == 0, 'frequency_per_day'] = links.loc[links['line_has_timetable'] == 0].apply(lambda x: np.ceil(3600*(x['nb_peak_hours']/x['headway_ph'] + (x['nb_service_hours'] - x['nb_peak_hours'])/x['headway_oph'])), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "ba168d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "links.loc[links['line_has_timetable']==1, 'headway'] = links.loc[links['line_has_timetable']==1, 'departures'].apply(retrieve_avg_headway)\n",
    "links.loc[links['line_has_timetable']==1, 'headway_ph'] = links.loc[links['line_has_timetable']==1, 'departures'].apply(retrieve_ph_headway)\n",
    "links.loc[links['line_has_timetable']==1, 'headway_oph'] = links.loc[links['line_has_timetable']==1, 'departures'].apply(retrieve_oph_headway)\n",
    "links.loc[links['line_has_timetable']==1, 'nb_service_hours'] = links.loc[links['line_has_timetable']==1, 'departures'].apply(retrieve_service_hours)\n",
    "links.loc[links['line_has_timetable']==1, 'nb_peak_hours'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "9e84f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "links['frequency (veh/hour)'] = 1/links['headway']*3600\n",
    "links['frequency ph (veh/hour)'] = 1/links['headway_ph']*3600\n",
    "links['frequency oph (veh/hour)'] = 1/links['headway_oph']*3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "b7a34049",
   "metadata": {},
   "outputs": [],
   "source": [
    "links['headway'] = links.apply(lambda x: x['headway'] if (x['headway_ph'] == x['headway_oph']) else None, 1)\n",
    "links['frequency (veh/hour)'] = links.apply(lambda x: x['frequency (veh/hour)'] if (x['frequency ph (veh/hour)'] == x['frequency oph (veh/hour)']) else None, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "3d8badd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hour = (links.groupby('route_id')['frequency (veh/hour)'].agg('mean')).to_dict()\n",
    "res_day = (links.groupby('route_id')['frequency_per_day'].agg('mean')).to_dict()\n",
    "\n",
    "df_route_id['frequency (veh/day)'] = res_day\n",
    "df_route_id['frequency (veh/hour)'] = res_hour\n",
    "\n",
    "if display_ph_columns:\n",
    "    res_ph = (links.groupby('route_id')['frequency ph (veh/hour)'].agg('mean')).to_dict()\n",
    "    res_oph = (links.groupby('route_id')['frequency oph (veh/hour)'].agg('mean')).to_dict()\n",
    "\n",
    "    df_route_id['frequency ph (veh/hour)'] = res_ph\n",
    "    df_route_id['frequency oph (veh/hour)'] = res_oph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "09422553",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hour = (links.groupby('trip_id')['frequency (veh/hour)'].agg('mean')).to_dict()\n",
    "res_day = (links.groupby('trip_id')['frequency_per_day'].agg('mean')).to_dict()\n",
    "\n",
    "df_trip_id['frequency (veh/day)'] = res_day\n",
    "df_trip_id['frequency (veh/hour)'] = res_hour\n",
    "\n",
    "if display_ph_columns:\n",
    "    res_ph = (links.groupby('trip_id')['frequency ph (veh/hour)'].agg('mean')).to_dict()\n",
    "    res_oph = (links.groupby('trip_id')['frequency oph (veh/hour)'].agg('mean')).to_dict()\n",
    "\n",
    "    df_trip_id['frequency ph (veh/hour)'] = res_ph\n",
    "    df_trip_id['frequency oph (veh/hour)'] = res_oph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "44a662c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hour = (links.groupby('route_type')['frequency (veh/hour)'].agg('mean')).to_dict()\n",
    "res_day = (links.groupby('route_type')['frequency_per_day'].agg('mean')).to_dict()\n",
    "\n",
    "df_route_type['frequency (veh/day)'] = res_day\n",
    "df_route_type['frequency (veh/hour)'] = res_hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a16eb3",
   "metadata": {},
   "source": [
    "# Line Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "bfa2e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparation. if length is NaN, or if shape dist travel exist.\n",
    "\n",
    "length_col = None\n",
    "if 'length' in links.columns and length_col == None:\n",
    "    if len(links[links['length'].isnull()])==0:\n",
    "        length_col = 'length'\n",
    "        \n",
    "if 'shape_dist_traveled' in links.columns and length_col == None:\n",
    "    if len(links[links['shape_dist_traveled'].isnull()])==0:\n",
    "        length_col = 'shape_dist_traveled'\n",
    "\n",
    "if length_col == None:\n",
    "    print('create length from geometry')\n",
    "    links['length'] = links.to_crs(crs).length\n",
    "    length_col = 'length'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638013ee",
   "metadata": {},
   "source": [
    "# Number of station per line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "6bf1bd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o-->o-->o-->o and  o<--o<--o<--o\n",
    "# est-ce que j'ai 8 ou 4 stations ?\n",
    "# j'ai 4 stations par trip et 4 stations par route (si c'est les memes).\n",
    "# comment savoir si cest les mêmes : clustering?\n",
    "# pour l'instant on prend tous les noeuds unique par route_id ou route_type (col='route_id', route_id)\n",
    "def get_num_station(col='route_id'):\n",
    "    link = links.groupby(col)[['a','b']].agg({'a':set,'b':set})\n",
    "    link['node_len'] = link.apply(lambda row: len(row['a'].union(row['b'])), axis=1)\n",
    "    return link['node_len'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "53a9fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "links['a_name'] = links['a'].map(nodes['stop_name'].to_dict())\n",
    "links['b_name'] = links['b'].map(nodes['stop_name'].to_dict())\n",
    "\n",
    "if len(nodes['stop_name'].values.tolist()) > len(nodes['stop_name'].unique()):\n",
    "    print('!! Duplicates in node names !!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "0eebf302",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_nb_trips = links[['route_id', 'trip_id']].drop_duplicates().groupby('route_id')['trip_id'].count().to_dict()\n",
    "df_route_id['type'] = df_route_id.index.map(dict_nb_trips)\n",
    "df_route_id['type'] = df_route_id['type'].apply(lambda x: 'circular' if x == 1 else 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "2165039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_sequence(route_id, trip_or_route = 'route'):\n",
    "    if trip_or_route == 'route':\n",
    "        if df_route_id.loc[route_id]['type'] == 'linear':\n",
    "            route_id = route_id + '_0'\n",
    "    links_route = links.loc[links.trip_id == route_id]\n",
    "    links_route = links_route.sort_values(by='link_sequence')\n",
    "    nodes_seq = []\n",
    "    for i in range(len(links_route)):\n",
    "        nodes_seq += [links_route.iloc[i]['a']]\n",
    "    nodes_seq += [links_route.iloc[-1]['b']]\n",
    "    return nodes_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "f19ccc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_stops = dict(zip(nodes.index, nodes['stop_name']))\n",
    "\n",
    "def get_stops_sequence(route_id, trip_or_route='route'):\n",
    "    nodes_seq = get_node_sequence(route_id, trip_or_route = trip_or_route)\n",
    "    stops_seq = []\n",
    "    for node in nodes_seq:\n",
    "        stops_seq += [nodes_stops[node]]\n",
    "    return stops_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "b44ac2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_route_id['stations sequence'] = [get_stops_sequence(route_id) for route_id in df_route_id.index]\n",
    "df_route_id['nodes sequence'] = [get_node_sequence(route_id) for route_id in df_route_id.index]\n",
    "df_route_id['nb stations'] = df_route_id['stations sequence'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "fb26cfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trip_id['stations sequence'] = [get_stops_sequence(trip_id, trip_or_route='trip') for trip_id in df_trip_id.index]\n",
    "df_trip_id['nodes sequence'] = [get_node_sequence(trip_id, trip_or_route='trip') for trip_id in df_trip_id.index]\n",
    "df_trip_id['nb stations'] = df_trip_id['stations sequence'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "ffa6009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_route_type = pd.DataFrame(df_route_id.groupby('route_type')['stations sequence'].agg(lambda x: list(set(sum(x, [])))))\n",
    "stations_route_type['nb stations'] = stations_route_type['stations sequence'].apply(lambda x: len(x))\n",
    "df_route_type = df_route_type.merge(stations_route_type, left_on=df_route_type.index, right_on=stations_route_type.index, how='left')\n",
    "df_route_type = df_route_type.rename(columns={'key_0': 'route_type'})\n",
    "df_route_type = df_route_type.set_index('route_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1ea85a",
   "metadata": {},
   "source": [
    "## Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "5549961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterable = list(zip(nodes['stop_name'], nodes.index))\n",
    "\n",
    "stops_nodes = defaultdict(set)\n",
    "for key, value in iterable:\n",
    "    stops_nodes[key].add(value)\n",
    "stops_nodes = dict(stops_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "2cacdef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterable = list(zip(links['a'], links['route_id']))\n",
    "iterable = iterable + list(zip(links['b'], links['route_id']))\n",
    "\n",
    "nodes_routes = defaultdict(set)\n",
    "for key, value in iterable:\n",
    "    nodes_routes[key].add(value)\n",
    "nodes_routes = dict(nodes_routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "d314d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_routes = {}\n",
    "\n",
    "for stop, node_list in stops_nodes.items():\n",
    "    routes = set()\n",
    "    for node in node_list:\n",
    "        if node in nodes_routes:\n",
    "            routes.update(nodes_routes[node])\n",
    "    stops_routes[stop] = routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "074016b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hubs = pd.DataFrame.from_dict(stops_routes, orient='index')\n",
    "hubs['lines'] = hubs.apply(lambda row: [val for val in row if pd.notnull(val)], axis=1)\n",
    "hubs = hubs.drop(columns=[i for i in range(len(hubs.columns) - 1)])\n",
    "hubs['nb_lines'] = hubs['lines'].apply(lambda x: len(x))\n",
    "hubs = hubs.sort_values(by='nb_lines', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "b639caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_route_type = dict(zip(df_route_id.index, df_route_id['route_type']))\n",
    "dict_veh = dict(zip(df_route_id['route_type'], df_route_id['veh_capacity (PAX)']))\n",
    "route_order = sorted(dict_veh, key=lambda x: int(dict_veh[x]), reverse=True)\n",
    "\n",
    "def lines_to_dict(lines):\n",
    "    route_dict = {route_type: [] for route_type in route_order}\n",
    "    for line in lines:\n",
    "        route_type = dict_route_type.get(line)\n",
    "        if route_type in route_dict:\n",
    "            route_dict[route_type].append(line)\n",
    "    route_dict = {k: sorted(v) for k, v in route_dict.items() if v}\n",
    "    return route_dict\n",
    "\n",
    "hubs['lines'] = hubs['lines'].apply(lines_to_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "792c180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.ops import unary_union\n",
    "\n",
    "def centroid(geometries):\n",
    "    combined_geometry = unary_union(geometries)\n",
    "    return combined_geometry.centroid\n",
    "\n",
    "centroids = pd.DataFrame(nodes.groupby('stop_name')['geometry'].agg(centroid))\n",
    "hubs = hubs.merge(centroids, left_on=hubs.index, right_on=centroids.index, how='left')\n",
    "\n",
    "hubs = hubs.rename(columns={'key_0': 'stop_name'})\n",
    "hubs = hubs.set_index('stop_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "feed3cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hubs['stop_radius'] = hubs['lines'].apply(lambda x: max(catchment_radius[mode] for mode in x.keys()))\n",
    "hubs['stop_radius'] = default_catchment_radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "ac08444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connections(row):\n",
    "    route_id = row.name\n",
    "    connections = set()\n",
    "    for station in row['stations sequence']:\n",
    "        if station in stops_routes:\n",
    "            connections.update(stops_routes[station])\n",
    "    connections.discard(route_id)  # Supprimer la route_id de l'ensemble des connexions\n",
    "    return lines_to_dict(connections), len(connections)\n",
    "\n",
    "df_route_id[['connexions', 'nb lines connected']] = df_route_id.apply(lambda row: pd.Series(get_connections(row)), axis=1)\n",
    "\n",
    "# df_route_id[['connexions']].loc[df_route_id['connexions'] == 'tertiary']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f98a64c",
   "metadata": {},
   "source": [
    "# Operating costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "c70257d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency = freq moy jour\n",
    "def get_veh_kmh(col='route_id', length_col='length'):\n",
    "    link = links.groupby([col, 'trip_id'])[[length_col, 'frequency_per_day', 'nb_service_hours']].agg({length_col:'sum', 'frequency_per_day': 'mean', 'nb_service_hours': 'mean'})\n",
    "    link['veh_km/h'] = np.ceil(link['frequency_per_day'] * link[length_col]) / 1000 / link['nb_service_hours'] #to km/H\n",
    "    return link.reset_index().groupby(col)['veh_km/h'].agg('sum').to_dict()\n",
    "\n",
    "def get_veh_km(col='route_id', length_col='length'):\n",
    "    link = links.groupby([col, 'trip_id'])[[length_col, 'frequency_per_day', 'nb_service_hours']].agg({length_col:'sum', 'frequency_per_day': 'mean', 'nb_service_hours': 'mean'})\n",
    "    link['veh_km/h'] = np.ceil(link['frequency_per_day'] * link[length_col]) / 1000  #to km/H\n",
    "    return link.reset_index().groupby(col)['veh_km/h'].agg('sum').to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "7677d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_veh_km('route_id', 'length')\n",
    "df_route_id['veh_km_day'] = res\n",
    "df_route_id['veh_km_year'] = df_route_id['veh_km_day'] * coef_day_to_year\n",
    "\n",
    "res = links.groupby('route_id')['capex'].agg('mean').to_dict()\n",
    "df_route_id['capex'] = res\n",
    "\n",
    "df_route_id['vehicle_cost_km_day'] = df_route_id['veh_km_day'] * df_route_id['capex']\n",
    "df_route_id['vehicle_cost_km_year'] = df_route_id['vehicle_cost_km_day'] * coef_day_to_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571b8861",
   "metadata": {},
   "source": [
    "# Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "765c4fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google_speed' in rlinks.columns:\n",
    "    rlinks['speed'] = rlinks['google_speed']\n",
    "    rlinks['time'] = rlinks['google_time']\n",
    "elif 'here_speed' in rlinks.columns:\n",
    "    rlinks['speed'] = rlinks['here_speed']\n",
    "    rlinks['time'] = rlinks['here_time']\n",
    "\n",
    "rlinks['speed'] = rlinks['speed'].astype(float)\n",
    "rlinks['time'] = rlinks['time'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "07e7b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ('road_link_list' in links.columns) and use_road_network:\n",
    "    line_links = links[['trip_id', 'road_link_list']]\n",
    "    road_link_list = line_links.explode('road_link_list').set_index('road_link_list')\n",
    "    road_link_list = road_link_list.merge(rlinks[['time', 'length', 'speed', 'osm_highway', 'geometry']], left_index=True, right_index=True, how='left')\n",
    "    road_link_list = gpd.GeoDataFrame(road_link_list.loc[road_link_list['osm_highway'] != 'train'], geometry='geometry', crs='EPSG:4326')\n",
    "    road_link_list.drop_duplicates(inplace=True)\n",
    "\n",
    "    res_times = road_link_list.groupby('trip_id')['time'].agg(np.nansum).to_dict()\n",
    "    df_trip_id['time'] = res_times\n",
    "\n",
    "    res_lengths = road_link_list.groupby('trip_id')['length'].agg(np.nansum).to_dict()\n",
    "    df_trip_id['length'] = res_lengths\n",
    "\n",
    "else:\n",
    "    res_times = links.groupby('trip_id')['time'].agg(np.nansum).to_dict()\n",
    "    df_trip_id['time'] = res_times\n",
    "\n",
    "    res_lengths = links.groupby('trip_id')['length'].agg(np.nansum).to_dict()\n",
    "    df_trip_id['length'] = res_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "e58de5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Time reduction parameters:\n",
    "res_inter_stop = links.groupby('trip_id')['stop_interval'].agg(np.nanmean).to_dict()\n",
    "df_trip_id['stop_interval'] = res_inter_stop\n",
    "res_road_pt_factor = links.groupby('trip_id')['road_pt_factor'].agg(np.nanmean).to_dict()\n",
    "df_trip_id['road_pt_factor'] = res_road_pt_factor\n",
    "\n",
    "# Computation\n",
    "df_trip_id['length (km)'] = df_trip_id['length']/1000\n",
    "\n",
    "df_trip_id['time raw (min)'] = df_trip_id['time']/60\n",
    "df_trip_id['time (min)'] = (df_trip_id['time']*df_trip_id['road_pt_factor'] + df_trip_id.apply(lambda x: max(0, x['nb stations']-2)*x['stop_interval'], axis=1))/60\n",
    "\n",
    "#df_trip_id['speed_raw'] = df_trip_id['length']/df_trip_id['time']*3.6\n",
    "df_trip_id['speed (km/h)'] = df_trip_id['length (km)']/(df_trip_id['time (min)']/60)\n",
    "\n",
    "df_trip_id.drop(columns=['stop_interval', 'road_pt_factor'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "1ad6ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = df_trip_id.groupby('route_id')['length'].agg(np.average).to_dict()\n",
    "df_route_id['length (km)'] = res\n",
    "df_route_id['length (km)'] /= 1000\n",
    "\n",
    "res = df_trip_id.groupby('route_id')['time (min)'].agg(np.average).to_dict()\n",
    "df_route_id['time (min)'] = res\n",
    "\n",
    "res = df_trip_id.groupby('route_id')['time (min)'].agg(np.nansum).to_dict()\n",
    "df_route_id['round trip time (s)'] = res\n",
    "df_route_id['round trip time (s)'] *= 60\n",
    "\n",
    "df_route_id['speed (km/h)'] = df_route_id['length (km)'] / (df_route_id['time (min)']/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a364cb6",
   "metadata": {},
   "source": [
    "## Operational Fleet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "ee5e14bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fleet_frequency(route_id):\n",
    "    time_min = df_route_id.loc[route_id, 'time (min)']\n",
    "    freq_ph = df_route_id.loc[route_id, 'frequency ph (veh/hour)']\n",
    "    return np.ceil(2 * freq_ph * time_min / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "8000a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fleet_timetable(route_id):\n",
    "    link = links.loc[links['route_id'] == route_id]\n",
    "    stations_sequence = df_route_id.loc[route_id]['nodes sequence']\n",
    "    circular_line = df_route_id.loc[route_id]['type'] == 'circular'\n",
    "\n",
    "    termini = [stations_sequence[0], stations_sequence[-1]]\n",
    "    link['a_terminus'] = link['a'].isin(termini)\n",
    "    link['b_terminus'] = link['b'].isin(termini)\n",
    "\n",
    "    if not circular_line:\n",
    "        dep0_times = link.loc[(link['trip_id'] == route_id + '_0') & link['a_terminus']]['departures'].values.tolist()[0]\n",
    "        dep1_times = link.loc[(link['trip_id'] == route_id + '_1') & link['a_terminus']]['departures'].values.tolist()[0]\n",
    "        arr0_times = link.loc[(link['trip_id'] == route_id + '_0') & link['b_terminus']]['arrivals'].values.tolist()[0]\n",
    "        arr1_times = link.loc[(link['trip_id'] == route_id + '_1') & link['b_terminus']]['arrivals'].values.tolist()[0]\n",
    "\n",
    "        dep0 = [time_seconds(t) for t in dep0_times]\n",
    "        dep1 = [time_seconds(t) for t in dep1_times]\n",
    "        arr0 = [time_seconds(t) for t in arr0_times]\n",
    "        arr1 = [time_seconds(t) for t in arr1_times]\n",
    "\n",
    "        travel_time0 = arr0[0] - dep0[0]\n",
    "        travel_time1 = arr1[0] - dep1[0]\n",
    "\n",
    "        nb_interbuses = []\n",
    "\n",
    "        for bus in dep0:\n",
    "            departure = bus\n",
    "            arrival = departure + travel_time0\n",
    "            if arrival < max(dep1):\n",
    "                departure_ = min([dep for dep in dep1 if dep>arrival])\n",
    "                tmax = departure_ + travel_time1\n",
    "            else:\n",
    "                tmax = max([max(dep0), max(arr1)])\n",
    "\n",
    "            t = bus\n",
    "            necessary_buses = 1\n",
    "            reserve_buses = 0\n",
    "            departures = [dep for dep in dep0 if dep>t]\n",
    "            arrivals = [arr for arr in arr1 if arr>t]\n",
    "\n",
    "            while t < tmax:\n",
    "                try:\n",
    "                    next_event = min([min(departures), min(arrivals)])\n",
    "                except ValueError:\n",
    "                    break\n",
    "                t=next_event\n",
    "                if (next_event in departures) and (next_event in arrivals):\n",
    "                    departures.pop(0)\n",
    "                    arrivals.pop(0)\n",
    "                    pass\n",
    "                elif next_event in departures:\n",
    "                    departures.pop(0)\n",
    "                    if reserve_buses >= 1:\n",
    "                        reserve_buses -= 1\n",
    "                    else:\n",
    "                        necessary_buses += 1\n",
    "                elif next_event in arrivals:\n",
    "                    arrivals.pop(0)\n",
    "                    necessary_buses += 1\n",
    "                    reserve_buses += 1\n",
    "\n",
    "            nb_interbuses.append(necessary_buses)\n",
    "\n",
    "        for bus in dep1:\n",
    "            departure = bus\n",
    "            arrival = departure + travel_time1\n",
    "            if arrival < max(dep0):\n",
    "                departure_ = min([dep for dep in dep0 if dep>arrival])\n",
    "                tmax = departure_ + travel_time0\n",
    "            else:\n",
    "                tmax = max([max(dep1), max(arr0)])\n",
    "\n",
    "            t = bus\n",
    "            necessary_buses = 1\n",
    "            reserve_buses = 0\n",
    "            departures = [dep for dep in dep1 if dep>t]\n",
    "            arrivals = [arr for arr in arr0 if arr>t]\n",
    "\n",
    "            while t < tmax:\n",
    "                try:\n",
    "                    next_event = min([min(departures), min(arrivals)])\n",
    "                except ValueError:\n",
    "                    break\n",
    "                t=next_event\n",
    "                if (next_event in departures) and (next_event in arrivals):\n",
    "                    departures.pop(0)\n",
    "                    arrivals.pop(0)\n",
    "                    pass\n",
    "                elif next_event in departures:\n",
    "                    departures.pop(0)\n",
    "                    if reserve_buses >= 1:\n",
    "                        reserve_buses -= 1\n",
    "                    else:\n",
    "                        necessary_buses += 1\n",
    "                elif next_event in arrivals:\n",
    "                    arrivals.pop(0)\n",
    "                    necessary_buses += 1\n",
    "                    reserve_buses += 1\n",
    "\n",
    "            nb_interbuses.append(necessary_buses)\n",
    "    \n",
    "    else:\n",
    "        dep0_times = link.loc[(link['trip_id'] == route_id + '_0') & link['a_terminus']]['departures'].values.tolist()[0]\n",
    "        arr0_times = link.loc[(link['trip_id'] == route_id + '_0') & link['b_terminus']]['arrivals'].values.tolist()[0]\n",
    "\n",
    "        dep0 = [time_seconds(t) for t in dep0_times]\n",
    "        arr0 = [time_seconds(t) for t in arr0_times]\n",
    "\n",
    "        travel_time0 = arr0[0] - dep0[0]\n",
    "\n",
    "        nb_interbuses = []\n",
    "        for bus in dep0:\n",
    "            departure = bus\n",
    "            arrival = departure + travel_time0\n",
    "            n_buses = len([dep for dep in dep0 if (dep>=bus and dep<arrival)])\n",
    "            nb_interbuses.append(n_buses)\n",
    "\n",
    "    return max(max(nb_interbuses), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "d1c4724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_service_hours_timetable(route_id):\n",
    "    link = links.loc[links['route_id'] == route_id]\n",
    "    stations_sequence = df_route_id.loc[route_id]['nodes sequence']\n",
    "    circular_line = df_route_id.loc[route_id]['type'] == 'circular'\n",
    "\n",
    "    termini = [stations_sequence[0], stations_sequence[-1]]\n",
    "    link['a_terminus'] = link['a'].isin(termini)\n",
    "    link['b_terminus'] = link['b'].isin(termini)\n",
    "\n",
    "    if not circular_line:\n",
    "        dep0_times = link.loc[(link['trip_id'] == route_id + '_0') & link['a_terminus']]['departures'].values.tolist()[0]\n",
    "        dep1_times = link.loc[(link['trip_id'] == route_id + '_1') & link['a_terminus']]['departures'].values.tolist()[0]\n",
    "        arr0_times = link.loc[(link['trip_id'] == route_id + '_0') & link['b_terminus']]['arrivals'].values.tolist()[0]\n",
    "        arr1_times = link.loc[(link['trip_id'] == route_id + '_1') & link['b_terminus']]['arrivals'].values.tolist()[0]\n",
    "\n",
    "        dep0 = [time_seconds(t) for t in dep0_times]\n",
    "        dep1 = [time_seconds(t) for t in dep1_times]\n",
    "        arr0 = [time_seconds(t) for t in arr0_times]\n",
    "        arr1 = [time_seconds(t) for t in arr1_times]\n",
    "\n",
    "        time_range = np.ceil((max([max(arr0), max(arr1)]) - min([min(dep0), min(dep1)]))/3600)\n",
    "\n",
    "    return time_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "6260419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "links.loc[links['line_has_timetable']==1, 'fleet'] = links.loc[links['line_has_timetable']==1, 'route_id'].apply(get_fleet_timetable)\n",
    "links.loc[links['line_has_timetable']==1, 'nb_service_hours'] = links.loc[links['line_has_timetable']==1, 'route_id'].apply(get_service_hours_timetable)\n",
    "\n",
    "links.loc[links['line_has_timetable']==0, 'fleet'] = links.loc[links['line_has_timetable']==0, 'route_id'].apply(get_fleet_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "f57cbffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = (links.groupby('trip_id')['nb_service_hours'].agg('mean')).to_dict()\n",
    "df_trip_id['nb_service_hours'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "f81d49ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = (links.groupby('route_id')['fleet'].agg('mean')).to_dict()\n",
    "df_route_id['fleet'] = res\n",
    "\n",
    "res = (links.groupby('route_id')['nb_service_hours'].agg('mean')).to_dict()\n",
    "df_route_id['nb_service_hours'] = res\n",
    "\n",
    "df_route_id['frequency (veh/hour)'] = df_route_id['frequency (veh/day)'] / df_route_id['nb_service_hours']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "1e683e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = df_route_id.reset_index().groupby('route_type')['fleet'].agg('sum').to_dict()\n",
    "df_route_type['fleet'] = res\n",
    "\n",
    "res = df_route_id.reset_index().groupby('route_type')['nb_service_hours'].agg('max').to_dict()\n",
    "df_route_type['nb_service_hours'] = res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7256e738",
   "metadata": {},
   "source": [
    "## Modal splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee00c44c",
   "metadata": {},
   "source": [
    "## Desserte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "b328d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if od_file_provided:\n",
    "    \n",
    "    nodes_with_zones_ = nodes_with_zones.copy().drop(columns='trip_ids').explode('route_ids')\n",
    "\n",
    "    if 'nom_com' in zonage.columns:\n",
    "        zone_to_com = zonage.set_index('zone_id')['nom_com'].to_dict()\n",
    "        nodes_with_zones['nom_com'] = nodes_with_zones['zone_id'].map(zone_to_com)\n",
    "        nodes_with_zones_['nom_com'] = nodes_with_zones_['zone_id'].map(zone_to_com)\n",
    "        serving_com = nodes_with_zones_.groupby('route_ids')['nom_com'].agg(set).to_dict()\n",
    "        df_route_id['served_coms'] = df_route_id.index.map(serving_com)\n",
    "        od['origin_com'] = od['origin'].map(zone_to_com)\n",
    "        od['destination_com'] = od['destination'].map(zone_to_com)\n",
    "\n",
    "    serving_zone = nodes_with_zones_.groupby('route_ids')['zone_id'].agg(set).to_dict()\n",
    "    df_route_id['served_zones'] = df_route_id.index.map(serving_zone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29df6f40",
   "metadata": {},
   "source": [
    "## Parts modales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "5ded46c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if od_file_provided:\n",
    "\n",
    "    if 'origin_com' in od.columns:\n",
    "        origin_field, destination_field, serving = 'origin_com', 'destination_com', serving_com\n",
    "    else: \n",
    "        origin_field, destination_field, serving = 'origin', 'destination', serving_zone\n",
    "\n",
    "    if od_file_provided:\n",
    "        volumes = [col for col in od.columns if 'volume_' in col]\n",
    "        if volumes:\n",
    "            dic_total_vol = {}\n",
    "            dic_vol = {}\n",
    "            dic_modal_share = {}\n",
    "            for line, zones in serving.items():\n",
    "                mode = df_route_id.loc[line]['route_type']\n",
    "                relevant_ods = od.copy()\n",
    "                if len(zones) >= 2:\n",
    "                    relevant_ods = relevant_ods.loc[relevant_ods[origin_field].isin(zones) & relevant_ods[destination_field].isin(zones) & (relevant_ods[origin_field] != relevant_ods[destination_field])]\n",
    "                else:\n",
    "                    relevant_ods = relevant_ods.loc[relevant_ods[origin_field].isin(zones) & relevant_ods[destination_field].isin(zones)]\n",
    "\n",
    "                try:\n",
    "                    dic_total_vol[mode][line] = relevant_ods['volume'].sum()\n",
    "                except KeyError:\n",
    "                    dic_total_vol[mode] = {}\n",
    "                    dic_total_vol[mode][line] = relevant_ods['volume'].sum()\n",
    "                if 'volume_{}'.format(mode) in volumes:\n",
    "                    try:\n",
    "                        dic_vol[mode][line] = relevant_ods['volume_{}'.format(mode)].sum()\n",
    "                        dic_modal_share[mode][line] = dic_vol[mode][line] / dic_total_vol[mode][line]  \n",
    "                    except KeyError:\n",
    "                        dic_vol[mode] = {}\n",
    "                        dic_vol[mode][line] = relevant_ods['volume_{}'.format(mode)].sum()\n",
    "                        dic_modal_share[mode] = {}\n",
    "                        dic_modal_share[mode][line] = dic_vol[mode][line] / dic_total_vol[mode][line]  \n",
    "\n",
    "            for mode in dic_modal_share:\n",
    "                df_route_id.loc[df_route_id['route_type'] == mode, 'modal share'] = df_route_id.loc[df_route_id['route_type'] == mode].index.map(dic_modal_share[mode])\n",
    "                df_route_type.loc[mode, 'modal share OD'] = sum(list(dic_vol[mode].values())) / sum(list(dic_total_vol[mode].values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9057e5",
   "metadata": {},
   "source": [
    "## Flux TC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "ed6570a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if od_file_provided:\n",
    "    modal_shares = links.groupby('route_type')['modal_share_input'].agg(np.nanmean).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "66f5a175",
   "metadata": {},
   "outputs": [],
   "source": [
    "if od_file_provided:\n",
    "\n",
    "    if 'origin_com' in od.columns:\n",
    "        origin_field, destination_field, serving = 'origin_com', 'destination_com', serving_com\n",
    "    else: \n",
    "        origin_field, destination_field, serving = 'origin', 'destination', serving_zone\n",
    "\n",
    "    if od_file_provided:\n",
    "        dic_total_vol_sans_filtre = {}\n",
    "        dic_modal_shares = {}\n",
    "        for line, zones in serving.items():\n",
    "\n",
    "            modal_share = modal_shares[links.loc[links['route_id'] == line, 'route_type'].values.tolist()[0]]\n",
    "            dic_modal_shares[line] = modal_share\n",
    "            \n",
    "            relevant_ods = od.copy()\n",
    "            if len(zones) >= 2:\n",
    "                relevant_ods = relevant_ods.loc[relevant_ods[origin_field].isin(zones) & relevant_ods[destination_field].isin(zones) & (relevant_ods[origin_field] != relevant_ods[destination_field])]\n",
    "            else:\n",
    "                relevant_ods = relevant_ods.loc[relevant_ods[origin_field].isin(zones) & relevant_ods[destination_field].isin(zones)]\n",
    "\n",
    "            dic_total_vol_sans_filtre[line] = relevant_ods['volume'].sum()*modal_share \n",
    "\n",
    "        df_route_id['modal_share_input'] = dic_modal_shares\n",
    "        df_route_id['volume'] = dic_total_vol_sans_filtre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "14434abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if od_file_provided:\n",
    "    if 'is_train' in od.columns:\n",
    "        if 'origin_com' in od.columns:\n",
    "            origin_field, destination_field, serving = 'origin_com', 'destination_com', serving_com\n",
    "        else: \n",
    "            origin_field, destination_field, serving = 'origin', 'destination', serving_zone\n",
    "\n",
    "        if od_file_provided:\n",
    "            dic_total_vol = {}\n",
    "            for line, zones in serving.items():\n",
    "\n",
    "                modal_share = modal_shares[links.loc[links['route_id'] == line, 'route_type'].values.tolist()[0]]\n",
    "                dic_modal_shares[line] = modal_share\n",
    "\n",
    "                relevant_ods = od.copy()\n",
    "                if len(zones) >= 2:\n",
    "                    relevant_ods = relevant_ods.loc[~relevant_ods['is_train'] & relevant_ods[origin_field].isin(zones) & relevant_ods[destination_field].isin(zones) & (relevant_ods[origin_field] != relevant_ods[destination_field])]\n",
    "                else:\n",
    "                    relevant_ods = relevant_ods.loc[relevant_ods[origin_field].isin(zones) & relevant_ods[destination_field].isin(zones)]\n",
    "\n",
    "                dic_total_vol[line] = relevant_ods['volume'].sum()*modal_share \n",
    "\n",
    "            df_route_id['capturable volume (no train)'] = dic_total_vol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5213bf",
   "metadata": {},
   "source": [
    "# Export results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7948cbd",
   "metadata": {},
   "source": [
    "## Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9250e0f1",
   "metadata": {},
   "source": [
    "### Characterstics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "37a39b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sec_to_duree(total_seconds):\n",
    "    hours = int(total_seconds // 3600)\n",
    "    minutes = int((total_seconds % 3600) // 60)\n",
    "    seconds = int(total_seconds % 60)\n",
    "    if seconds >= 30:\n",
    "        minutes += 1\n",
    "    if minutes == 60:\n",
    "        hours += 1\n",
    "        minutes = 0\n",
    "    time_str = \"\"\n",
    "    if hours > 0:\n",
    "        time_str += f\"{hours}h \"\n",
    "    if minutes > 0:  \n",
    "        time_str += f\"{minutes}min \"\n",
    "    return time_str.strip()\n",
    "\n",
    "df_route_id[\"round_trip_time\"] = df_route_id[\"round trip time (s)\"].apply(sec_to_duree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "3a1861cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['route_type', \n",
    "           'nb stations', \n",
    "           'length (km)',\n",
    "           'round_trip_time',\n",
    "           'speed (km/h)', \n",
    "           'frequency (veh/day)', \n",
    "           'veh_capacity (PAX)', \n",
    "           'fleet',\n",
    "           'veh_km_year', \n",
    "           'vehicle_cost_km_year',\n",
    "           'geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "40caa102",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_table = df_route_id.copy()[columns]\n",
    "char_table['length (km)'] = char_table['length (km)'].apply(np.round, decimals=1)\n",
    "char_table['speed (km/h)'] = char_table['speed (km/h)'].apply(np.round, decimals=0)\n",
    "char_table['veh_km_year'] = char_table['veh_km_year'].apply(np.round, decimals=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "350c24ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_table.drop(columns='geometry').to_csv(output_folder + 'lines_chacteristics.csv')\n",
    "\n",
    "char_table_geo = gpd.GeoDataFrame(char_table, geometry='geometry', crs='EPSG:4326')\n",
    "char_table_geo.explode().to_file(output_folder + 'lines_chacteristics.geojson', driver='GeoJSON', engine=io_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d395489c",
   "metadata": {},
   "source": [
    "### Catchment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "e357b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_columns = [col for col in df_route_id.columns if 'catchment' in col]\n",
    "catchment_columns_ = catchment_columns + ['geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "e0d85139",
   "metadata": {},
   "outputs": [],
   "source": [
    "catch_table = df_route_id.copy()[catchment_columns_]\n",
    "for col in catchment_columns:\n",
    "    catch_table[col] = catch_table[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "cddfc42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "catch_table.drop(columns='geometry').to_csv(output_folder + 'lines_catchment.csv')\n",
    "\n",
    "catch_table_geo = gpd.GeoDataFrame(catch_table, geometry='geometry', crs='EPSG:4326')\n",
    "catch_table_geo.explode().to_file(output_folder + 'lines_catchment.geojson', driver='GeoJSON', engine=io_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "37b8d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : formater les tableaux de sortie de df_route_id ==> caractéristiques / accessibilité / réponse au besoin\n",
    "\n",
    "# Dans réponse au besoin : estimation du volume de flux TC desservis sans correspondance pour une PM de 20%, estimation du taux de remplissage TC sans correspondance pour une PM TC de 10%\n",
    "\n",
    "#TODO: ajouter les tableaux globaux df_route_type (longueur totale, nombre de stations, flotte, veh.km/jour, capex/jour, veh.km/an, capex/an) et hubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "4a7d0d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# round numbers\n",
    "#TODO : change label catchment\n",
    "# for col in ['catchment population', 'frequency (veh/hours)','length (m)','veh.km/h','round trip time (s)']:\n",
    "#     df_route_id[col] = df_route_id[col].apply(lambda x :np.round(x,2))\n",
    "#     df_route_id[col] = df_route_id[col].apply(lambda x :np.round(x,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "3105f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_route_id = df_route_id.fillna('null')\n",
    "#df_route_type = df_route_type.fillna('null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "25e52545",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_route_id['length (km)'] = df_route_id['length (km)'].apply(np.round, decimals=1)\n",
    "df_route_id['speed (km/h)'] = df_route_id['speed (km/h)'].apply(np.round, decimals=1)\n",
    "df_route_id['veh_km_year'] = df_route_id['veh_km_year'].apply(np.round, decimals=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "984694c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_route_id.drop(columns=['geometry', 'nodes sequence']).to_csv(output_folder + 'route_id_metrics.csv')\n",
    "# df_route_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "c6a5c283",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_route_type.drop(columns='stations sequence').to_csv(output_folder + 'route_type_metrics.csv')\n",
    "# df_route_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161f1ed8",
   "metadata": {},
   "source": [
    "## Modal share table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "38be99cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if od_file_provided:\n",
    "    \n",
    "    modal_table = df_route_id[['modal share', 'volume', 'capturable volume (no train)', 'geometry']]\n",
    "    modal_table['modal share (%)'] = modal_table['modal share'].apply(lambda x: np.round(100*x, 1))\n",
    "    modal_table['volume'] = modal_table['volume'].apply(np.round, decimals=0)\n",
    "    modal_table['capturable volume (no train)'] = modal_table['capturable volume (no train)'].apply(np.round, decimals=0)\n",
    "\n",
    "    modal_table.drop(columns=['geometry', 'modal share']).to_csv(output_folder + 'line_flows.csv')\n",
    "\n",
    "    modal_table = gpd.GeoDataFrame(modal_table, geometry='geometry', crs='EPSG:4326')\n",
    "    modal_table.explode().to_file(output_folder + 'line_flows.geojson', driver='GeoJSON', engine=io_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1a948d",
   "metadata": {},
   "source": [
    "## Geomatic outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dcea6b",
   "metadata": {},
   "source": [
    "Hubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "395318b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hubs_plot = hubs.copy()\n",
    "hubs_plot['lines'] = hubs_plot['lines'].apply(lambda x: str(x).replace(',', ';').replace(\"'\", '')[1:-1])\n",
    "hubs_plot.drop(columns='geometry').to_csv(output_folder + 'hubs.csv')\n",
    "\n",
    "hubs = gpd.GeoDataFrame(hubs, geometry='geometry', crs='EPSG:4326')\n",
    "hubs.to_file(output_folder + 'hubs.geojson', driver='GeoJSON', engine=io_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b22e89",
   "metadata": {},
   "source": [
    "Common sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "2cf546b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renvoie un fichier geojson avec les tronçons en commun entre plusieurs lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "8ad927d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering de 500m pour a et b et cluster d'appartenance\n",
    "coords_nodes = np.array(nodes['geometry'].apply(lambda point: (point.x, point.y)).tolist())\n",
    "\n",
    "# Convertir 500 mètres en degrés : 111 km = 1 degré de latitude\n",
    "eps_lat = clustering_radius / (111 * 1000)  # Environ 0.0045 degrés\n",
    "\n",
    "# 1 degré de longitude dépend de la latitude\n",
    "mean_latitude = np.mean(coords_nodes[:, 1])\n",
    "eps_lon = clustering_radius / (111 * 1000 * np.cos(np.radians(mean_latitude)))\n",
    "\n",
    "# Appliquer DBSCAN avec une distance euclidienne pondérée\n",
    "db = DBSCAN(eps=1, min_samples=1, metric='euclidean').fit(coords_nodes / [eps_lon, eps_lat])\n",
    "\n",
    "# Ajouter les labels de cluster au GeoDataFrame\n",
    "nodes['cluster'] = db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "de33553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = nodes.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "605b754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = links.merge(nodes[['index', 'cluster']], left_on='a', right_on='index', how='left').drop(columns='index').rename(columns={'cluster': 'a_clustered'})\n",
    "links = links.merge(nodes[['index', 'cluster']], left_on='b', right_on='index', how='left').drop(columns='index').rename(columns={'cluster': 'b_clustered'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "43462dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_troncons = links.groupby(['a_clustered', 'b_clustered'])['route_id'].agg(list).reset_index()\n",
    "l_troncons['nb_lines'] = l_troncons['route_id'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "7a122690",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_troncons_communs = l_troncons[l_troncons.nb_lines > 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "2f63f1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "troncons_communs = len(l_troncons_communs) > 0.\n",
    "print(troncons_communs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "77eaadb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if troncons_communs:\n",
    "    l_troncons_communs['geometry'] = l_troncons_communs.apply(\n",
    "        lambda row : links[(links.a_clustered == row['a_clustered']) & (links.b_clustered == row['b_clustered'])].drop_duplicates(subset=['a_clustered', 'b_clustered'], keep='first').geometry.values[0],\n",
    "        axis=1\n",
    "        )\n",
    "    l_troncons_communs['stations_a'] = l_troncons_communs['a_clustered'].apply(lambda x: list(nodes[nodes.cluster == x]['stop_name'].unique()))\n",
    "    l_troncons_communs['stations_b'] = l_troncons_communs['b_clustered'].apply(lambda x: list(nodes[nodes.cluster == x]['stop_name'].unique()))\n",
    "    l_troncons_communs = gpd.GeoDataFrame(l_troncons_communs[['stations_a', 'stations_b', 'route_id', 'nb_lines', 'geometry']], geometry='geometry', crs='EPSG:4326')\n",
    "    l_troncons_communs.to_file(output_folder + 'pt_common_sections.geojson', driver='GeoJSON', engine=io_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372bb5bd",
   "metadata": {},
   "source": [
    "df_route_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "13abf2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.GeoDataFrame(df_route_id, geometry='geometry', crs='EPSG:4326').to_file(output_folder + 'pt_network_kpis.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6559846b",
   "metadata": {},
   "source": [
    "Nodes catchment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "01bc6f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO pcq c'est DYNAMIQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "10166c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using get catchment : get the catchment radius of each node (get larger one if used by many modes)\n",
    "for col in catchment_radii:\n",
    "    suf = col.split('catchment_radius_')[1]\n",
    "    for density in densities:\n",
    "        if density == 'density' and 'population_density' not in densities:\n",
    "            tag = 'population'\n",
    "        elif density == 'density':\n",
    "            tag = 'x'\n",
    "        else:\n",
    "            tag = density.split('_density')[0]\n",
    "\n",
    "        mesh, node_dist = meshes[tag], node_dists[tag]\n",
    "\n",
    "        link = links.groupby('route_type')[['a', 'b', 'route_type', col]].agg({'a': set, 'b': set, 'route_type': 'first', col:np.nanmean})\n",
    "        link['node'] = link.apply(lambda row: row['a'].union(row['b']), axis=1)\n",
    "        link = link.drop(columns=['a','b'])\n",
    "        ## add catchment radius for the route_type\n",
    "        link = link.explode('node').reset_index(drop=True)\n",
    "        link = link.sort_values(col,ascending=False).drop_duplicates('node',keep='first')\n",
    "        link = node_dist.merge(link, left_on='node_index', right_on='node')\n",
    "        link = link[link['distances'] <= link[col]]\n",
    "\n",
    "        temp_dict = link.groupby('node_index')[tag].sum().to_dict()\n",
    "        nodes['catchment {} {}'.format(suf, tag)] = nodes['index'].map(temp_dict.get)\n",
    "\n",
    "        temp_dict = link.groupby('node_index')[col].agg('first').to_dict() \n",
    "        nodes[col] = nodes['index'].map(temp_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "4460bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.to_file(output_folder + 'nodes.geojson', driver='GeoJSON', engine=io_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204675d1",
   "metadata": {},
   "source": [
    "## Graphs and pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "ba0c4f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot = df_route_type.reset_index().plot(kind='bar', x='route_type', y='catchment', color='#559bb4', rot=0, figsize=[10, 5])\n",
    "# plot.set_title('Couverture population par mode')\n",
    "# plot.set_ylabel('')\n",
    "# plot.set_xlabel(\"route_type\")\n",
    "# plot.legend([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
